# LLMPlan ê¸°ìˆ  ëª…ì„¸ì„œ

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ë ˆì´ì–´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Presentation Layer          â”‚  â† FastAPI Routes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Application Layer           â”‚  â† Use Cases, DTOs
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Domain Layer              â”‚  â† Entities, Services
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Infrastructure Layer         â”‚  â† LMStudio Repository
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° í”Œë¡œìš°

```
HTTP Request â†’ Router â†’ Use Case â†’ Domain Service â†’ Repository â†’ LMStudio
                â†“
HTTP Response â† DTO â† Domain Entity â† Summary â† LLM Response
```

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. Domain Entities

```python
# Summary Entity
class Summary:
    id: str
    original_text: str
    summary_text: str
    created_at: datetime
    model_name: str
    summary_length: int
```

### 2. Value Objects

```python
# Summary Configuration
class SummaryConfig:
    max_tokens: int = 1000
    temperature: float = 0.3
    model_name: str = "qwen/qwen3-4b"
    summary_type: Literal["concise", "detailed", "bullet_points"]
    language: Literal["korean", "english"]

# LMStudio Configuration
class LMStudioConfig:
    base_url: str = "http://localhost:1234/v1"
    api_key: str = "lm-studio"
    timeout: int = 30
    max_retries: int = 3
```

### 3. Repository Interface

```python
class SummaryRepository(ABC):
    async def summarize_text(self, text: str, config: SummaryConfig) -> Summary
    async def health_check(self) -> bool
```

### 4. Use Cases

```python
class SummarizeTextUseCase:
    async def execute(self, request: SummaryRequest) -> SummaryResponse

class HealthCheckUseCase:
    async def execute(self, request: HealthCheckRequest) -> HealthCheckResponse
```

## ğŸ”Œ ì™¸ë¶€ ì—°ë™

### LMStudio API

```python
# ì—°ê²° ì„¤ì •
llm = ChatOpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio",
    model="qwen/qwen3-4b",
    max_tokens=1000,
    temperature=0.3
)

# ìš”ì•½ ìš”ì²­
messages = [
    SystemMessage(content=system_prompt),
    HumanMessage(content=f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{text}")
]
response = await llm.ainvoke(messages)
```

### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

```python
def _get_system_prompt(self, config: SummaryConfig) -> str:
    base_prompt = "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ í…ìŠ¤íŠ¸ ìš”ì•½ AIì…ë‹ˆë‹¤."

    if config.summary_type == "concise":
        return f"{base_prompt} í•µì‹¬ ë‚´ìš©ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."
    elif config.summary_type == "detailed":
        return f"{base_prompt} ì£¼ìš” ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."
    elif config.summary_type == "bullet_points":
        return f"{base_prompt} ì£¼ìš” í¬ì¸íŠ¸ë¥¼ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
```

## ğŸ“Š ë°ì´í„° ëª¨ë¸

### Request/Response DTOs

```python
# ìš”ì•½ ìš”ì²­
class SummaryRequest(BaseModel):
    text: str = Field(min_length=10, max_length=50000)
    summary_type: Literal["concise", "detailed", "bullet_points"] = "concise"
    language: Literal["korean", "english"] = "korean"
    max_tokens: int = Field(default=1000, ge=100, le=4000)
    temperature: float = Field(default=0.3, ge=0.0, le=2.0)

# ìš”ì•½ ì‘ë‹µ
class SummaryResponse(BaseModel):
    id: str
    original_text: str
    summary_text: str
    created_at: datetime
    model_name: str
    summary_length: int
    original_length: int
    compression_ratio: float
```

### ì—ëŸ¬ ëª¨ë¸

```python
class ErrorResponse(BaseModel):
    error: str
    error_code: str
    timestamp: datetime
    details: Optional[str] = None
```

## ğŸ”’ ë³´ì•ˆ ë° ê²€ì¦

### ì…ë ¥ ê²€ì¦

- **í…ìŠ¤íŠ¸ ê¸¸ì´**: 10-50,000ì
- **íŒŒë¼ë¯¸í„° ë²”ìœ„**: max_tokens(100-4000), temperature(0.0-2.0)
- **í—ˆìš©ëœ ê°’**: summary_type, language enum ê²€ì¦
- **XSS ë°©ì§€**: HTML íƒœê·¸ ì œê±°/ì´ìŠ¤ì¼€ì´í”„

### ì—ëŸ¬ ì²˜ë¦¬

```python
try:
    summary = await self.summary_service.summarize_text(text, config)
    return SummaryResponse.from_domain_entity(summary)
except ValueError as e:
    raise HTTPException(400, detail=ErrorResponse(...))
except RuntimeError as e:
    raise HTTPException(500, detail=ErrorResponse(...))
except Exception as e:
    raise HTTPException(500, detail=ErrorResponse(...))
```

## ğŸ³ ì»¨í…Œì´ë„ˆí™”

### Dockerfile íŠ¹ì§•

```dockerfile
FROM python:3.12-slim
WORKDIR /app

# uvë¥¼ ì‚¬ìš©í•œ ë¹ ë¥¸ ì˜ì¡´ì„± ì„¤ì¹˜
RUN pip install uv
COPY pyproject.toml uv.lock ./
RUN uv sync --no-dev

# ë³´ì•ˆ: non-root ì‚¬ìš©ì
RUN useradd --create-home --shell /bin/bash app
USER app

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s \
  CMD curl -f http://localhost:8000/api/v1/summary/health
```

### ë©€í‹°í”Œë«í¼ ì§€ì›

- **linux/amd64**: Intel/AMD ì„œë²„
- **linux/arm64**: Apple Silicon, ARM ì„œë²„

## â˜¸ï¸ Kubernetes ë°°í¬

### Helm Chart êµ¬ì¡°

```
charts/llmplan/
â”œâ”€â”€ Chart.yaml          # ì°¨íŠ¸ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ values.yaml         # ê¸°ë³¸ê°’ ì„¤ì •
â””â”€â”€ templates/
    â”œâ”€â”€ deployment.yaml  # Pod ë°°í¬
    â”œâ”€â”€ service.yaml     # ì„œë¹„ìŠ¤ ë…¸ì¶œ
    â”œâ”€â”€ configmap.yaml   # í™˜ê²½ ì„¤ì •
    â”œâ”€â”€ ingress.yaml     # ì™¸ë¶€ ì ‘ê·¼ (ì„ íƒ)
    â”œâ”€â”€ hpa.yaml         # ìë™ ìŠ¤ì¼€ì¼ë§ (ì„ íƒ)
    â””â”€â”€ lmstudio-service.yaml  # LMStudio ì—°ë™
```

### ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­

```yaml
resources:
  requests:
    cpu: 500m # 0.5 CPU ì½”ì–´
    memory: 512Mi # 512MB RAM
  limits:
    cpu: 1000m # 1 CPU ì½”ì–´
    memory: 1Gi # 1GB RAM
```

### ìŠ¤ì¼€ì¼ë§ ì„¤ì •

```yaml
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ

```
        E2E Tests (Manual)
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     Integration Tests (3)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  Unit Tests (33) - CI Safe
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
```

### CI-Safe í…ŒìŠ¤íŠ¸ (36ê°œ)

- **Domain Logic**: ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
- **Value Objects**: ì„¤ì • ê²€ì¦
- **DTOs**: ì§ë ¬í™”/ì—­ì§ë ¬í™”
- **Mocked Integration**: LMStudio ëª¨í‚¹

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í™˜ê²½

- **Local**: ëª¨ë“  í…ŒìŠ¤íŠ¸ (ì‹¤ì œ LMStudio í•„ìš”)
- **CI**: CI-Safe í…ŒìŠ¤íŠ¸ë§Œ (ëª¨í‚¹ ì‚¬ìš©)
- **Docker**: ì»¨í…Œì´ë„ˆ ë‚´ í…ŒìŠ¤íŠ¸

## ğŸ”„ ê°œë°œ ì›Œí¬í”Œë¡œìš°

### ë¡œì»¬ ê°œë°œ

```bash
# í™˜ê²½ ì„¤ì •
uv sync
source .venv/bin/activate

# LMStudio ì‹œì‘ (ë³„ë„)
# Qwen 3-4B ëª¨ë¸ ë¡œë“œ

# ê°œë°œ ì„œë²„ ì‹¤í–‰
uv run uvicorn app.main:app --reload

# í…ŒìŠ¤íŠ¸
uv run pytest
```

### ë°°í¬ ì›Œí¬í”Œë¡œìš°

```
Code Push â†’ CI Tests â†’ Docker Build â†’ Docker Push â†’ Helm Package â†’ OCI Push
```

### ë¸Œëœì¹˜ ì „ëµ

- **main**: í”„ë¡œë•ì…˜ ì¤€ë¹„ ì½”ë“œ
- **feature/\***: ê¸°ëŠ¥ ê°œë°œ
- **hotfix/\***: ê¸´ê¸‰ ìˆ˜ì •

## ğŸ“ ì½”ë”© ì»¨ë²¤ì…˜

### Python ìŠ¤íƒ€ì¼

- **Formatter**: Ruff
- **Line Length**: 120ì
- **Import Order**: isort ê·œì¹™
- **Type Hints**: ëª¨ë“  í•¨ìˆ˜/ë©”ì„œë“œ

### ë„¤ì´ë° ì»¨ë²¤ì…˜

- **Classes**: PascalCase (`SummaryService`)
- **Functions**: snake_case (`summarize_text`)
- **Constants**: UPPER_SNAKE_CASE (`DEFAULT_MODEL_NAME`)
- **Files**: snake_case (`summary_service.py`)

### ë¬¸ì„œí™”

- **Docstrings**: Google ìŠ¤íƒ€ì¼
- **Type Hints**: ëª¨ë“  public ì¸í„°í˜ì´ìŠ¤
- **API Docs**: FastAPI ìë™ ìƒì„± (/docs)

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. LMStudio ì—°ê²° ì‹¤íŒ¨

```bash
# ì¦ìƒ: HTTP 503, "LMStudio service is not responding"
# í•´ê²°: LMStudio ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:1234/v1/models
```

#### 2. ìš”ì•½ í’ˆì§ˆ ë¬¸ì œ

```bash
# ì¦ìƒ: ì´ìƒí•œ ìš”ì•½ ê²°ê³¼
# í•´ê²°: temperature, max_tokens ì¡°ì •
# temperature: 0.1-0.7 (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ)
# max_tokens: 500-2000 (ê¸¸ìˆ˜ë¡ ìƒì„¸í•¨)
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# ì¦ìƒ: OOM Killed
# í•´ê²°: Kubernetes ë¦¬ì†ŒìŠ¤ ì¦ê°€
helm upgrade llmplan oci://registry-1.docker.io/eunheejo/llmplan \
  --set resources.limits.memory=2Gi
```

### ë””ë²„ê¹… ë„êµ¬

```bash
# ë¡œê·¸ í™•ì¸
kubectl logs -f deployment/llmplan -n llmplan

# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/api/v1/summary/health

# ë©”íŠ¸ë¦­ (í–¥í›„)
curl http://localhost:8000/metrics
```
