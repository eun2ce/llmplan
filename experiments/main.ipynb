{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a4af1f-793f-4ff9-9da0-2002f8b8212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers>=4.53.2 mlflow==3.2.0 accelerate qwen_vl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d957315-49f4-4cd6-80db-1d2fa14d44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, io, json, tempfile, requests\n",
    "from PIL import Image\n",
    "import torch, mlflow\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "import accelerate\n",
    "from importlib import import_module\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"   # HF가 TensorFlow 백엔드 불러오지 않도록\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # TF C++ 로그 숨김 (0~3: info~error)\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"      # (설치되어 있다면) JAX도 CPU만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95d3828-f1ae-48ee-bb48-7de3170e2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI = http://mlflow.mlflow.svc.cluster.local:5000\n"
     ]
    }
   ],
   "source": [
    "MLFLOW_URI = \"http://mlflow.mlflow.svc.cluster.local:5000\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_URI\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "print(\"Tracking URI =\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97766e2-c9d5-4496-8999-3e4ec54f3c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/2', creation_time=1755135394244, experiment_id='2', last_update_time=1755135394244, lifecycle_stage='active', name='qwen25vl_demo-eun2ce-20250814-01', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_NAME = \"qwen25vl_demo-eun2ce-20250814-01\"\n",
    "RUN_NAME = \"qwen25vl_smoketest-eun2ce-20250814-01\"\n",
    "# HF_MODEL_ID = \"\"\n",
    "MODEL_NAME = \"/models/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5\"\n",
    "\n",
    "mlflow.set_experiment(EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8065207e-7312-4326-9dfb-48f9e22fcdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_URL = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_images/resolve/main/COCO/000000039769.png\"\n",
    "IMAGE_PATH = \"./testimg.png\"\n",
    "\n",
    "PROMPT = \"이 이미지를 한 문장으로 설명해줘. 그리고 주요 객체 3개도 bullet로 적어줘.\"\n",
    "\n",
    "def load_image(image_url=None, image_path=None):\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        return Image.open(image_path).convert(\"RGB\")\n",
    "    if image_url:\n",
    "        r = requests.get(image_url, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "    raise ValueError(\"image_url 또는 image_path 중 하나는 반드시 지정해야 합니다.\")\n",
    "\n",
    "image = load_image(image_path=IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5626614-d8b2-4c65-afee-9b39750b4a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82da6853164b4b20b03114ab7e7fe75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = os.getenv(\"HF_MODEL_ID\", MODEL_NAME)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "dtype = torch.bfloat16 if use_gpu else torch.float32\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "processor  = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model      = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if use_gpu else None\n",
    ")\n",
    "# Qwen이 배포한 멀티모달 헬퍼 로드\n",
    "qwen_utils = import_module(\"qwen_vl_utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194ee0ba-810e-410b-b1d6-904784a218ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion: system ...\n",
      "latency=3.08s  tokens=198\n"
     ]
    }
   ],
   "source": [
    "def qwen_vl_generate(image: Image.Image, prompt: str, max_new_tokens=128, do_sample=False):\n",
    "    # Qwen 포맷의 chat 메시지\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    # text 프롬프트 생성\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # vision 입력 분리\n",
    "    image_inputs, video_inputs = qwen_utils.process_vision_info(messages)\n",
    "    # processor로 텐서화\n",
    "    inputs = processor(\n",
    "        text=[text], images=image_inputs, videos=video_inputs,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    if use_gpu:\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    t0 = time.time()\n",
    "    gen_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample\n",
    "    )\n",
    "    latency = time.time() - t0\n",
    "\n",
    "    # 첫 토큰부터 전체 디코딩 (Qwen의 special token은 스킵)\n",
    "    out = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "    # apply_chat_template가 앞부분까지 포함하므로, 마지막 응답만 잘라내기(간단 파싱)\n",
    "    # 대개 마지막 \"\\nassistant\\n\" 이후가 모델 응답입니다. 안전하게 뒤쪽 1200자만 사용.\n",
    "    completion = out[-1200:].strip()\n",
    "    return completion, latency, len(gen_ids[0]) if hasattr(gen_ids, \"__len__\") else None\n",
    "\n",
    "completion, latency, gen_tokens = qwen_vl_generate(image, PROMPT)\n",
    "print(\"completion:\", completion.splitlines()[0][:120], \"...\")\n",
    "print(f\"latency={latency:.2f}s  tokens={gen_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102cf55d-959c-46ae-b7dd-f7e9b98ffc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "import base64\n",
    "\n",
    "with open(IMAGE_PATH, \"rb\") as f:\n",
    "    img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "# 입력/출력 예시 생성 (PyFunc용)\n",
    "input_example = {\n",
    "    \"prompt\": \"이미지 속 장면을 요약해줘.\",\n",
    "    \"image_b64\": img_b64,\n",
    "    \"max_new_tokens\": 128\n",
    "}\n",
    "signature = infer_signature(\n",
    "    model_input=pd.DataFrame([input_example]),\n",
    "    model_output=pd.DataFrame([{\"completion\":\"text\"}])\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch, gc, time, pandas as pd\n",
    "\n",
    "class QwenVL_Pyfunc(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.model_id = context.model_config.get(\"model_id\", \"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "        # 가벼운 것만 준비(모두 CPU)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, trust_remote_code=True)\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_id, trust_remote_code=True)\n",
    "        # 모델 로드 옵션은 저장만 해두고, 진짜 로드는 predict에서 매 호출마다!\n",
    "        self._torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        self._device_map = \"auto\" if torch.cuda.is_available() else None\n",
    "\n",
    "    def _load_image(self, row):\n",
    "        from PIL import Image\n",
    "        import io, requests, base64, os\n",
    "        if \"image_path\" in row and isinstance(row[\"image_path\"], str) and os.path.exists(row[\"image_path\"]):\n",
    "            return Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        if \"image_b64\" in row and row[\"image_b64\"]:\n",
    "            return Image.open(io.BytesIO(base64.b64decode(row[\"image_b64\"]))).convert(\"RGB\")\n",
    "        if \"image_url\" in row and row[\"image_url\"]:\n",
    "            r = requests.get(row[\"image_url\"], timeout=15)\n",
    "            r.raise_for_status()\n",
    "            return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "        raise ValueError(\"image_path / image_url / image_b64 중 하나가 필요합니다.\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        # 매 호출마다 “잠깐” GPU로 로드\n",
    "        return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            self.model_id,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=self._torch_dtype,\n",
    "            device_map=self._device_map,\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # 입력 정규화\n",
    "        if isinstance(model_input, dict):\n",
    "            rows = [model_input]\n",
    "        elif isinstance(model_input, pd.DataFrame):\n",
    "            rows = model_input.to_dict(orient=\"records\")\n",
    "        else:\n",
    "            raise TypeError(\"predict input은 dict 또는 pandas.DataFrame 이어야 합니다.\")\n",
    "\n",
    "        model = self._build_model()\n",
    "        outs = []\n",
    "        try:\n",
    "            for row in rows:\n",
    "                prompt = row.get(\"prompt\", \"\")\n",
    "                image  = self._load_image(row)\n",
    "\n",
    "                # Qwen 멀티모달 입력 준비\n",
    "                from importlib import import_module\n",
    "                qwen_utils = import_module(\"qwen_vl_utils\")\n",
    "\n",
    "                messages = [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\",  \"text\": prompt}\n",
    "                    ]\n",
    "                }]\n",
    "                text = self.tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                image_inputs, video_inputs = qwen_utils.process_vision_info(messages)\n",
    "                inputs = self.processor(text=[text], images=image_inputs, videos=video_inputs,\n",
    "                                        return_tensors=\"pt\")\n",
    "\n",
    "                # 필요하면만 디바이스로 이동\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "                t0 = time.time()\n",
    "                with torch.inference_mode():\n",
    "                    gen_ids = model.generate(**inputs, max_new_tokens=int(row.get(\"max_new_tokens\", 128)))\n",
    "                latency = time.time() - t0\n",
    "\n",
    "                # 출력 후단 파싱(간단)\n",
    "                completion = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "                completion = completion[-1200:].strip()\n",
    "                outs.append({\"completion\": completion, \"latency\": latency})\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                del model\n",
    "            except Exception:\n",
    "                pass\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.ipc_collect()\n",
    "\n",
    "        return pd.DataFrame(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a30cc0d1-2113-4daa-af19-a29098116909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/14 03:15:43 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "2025/08/14 03:15:44 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025/08/14 03:15:44 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c394fa543afc4c58851b805970291c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID   : 87c49955790d47dab31d4919bef18822\n",
      "Model URI: mlflow-artifacts:/2/87c49955790d47dab31d4919bef18822/artifacts/model\n",
      "🏃 View run qwen25vl_smoketest-eun2ce-20250814-01 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/2/runs/87c49955790d47dab31d4919bef18822\n",
      "🧪 View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    mlflow.log_params({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"dtype\": str(dtype),\n",
    "        \"max_new_tokens\": 128,\n",
    "    })\n",
    "    mlflow.log_metrics({\n",
    "        \"latency_s\": latency,\n",
    "        \"generated_tokens\": gen_tokens or 0,\n",
    "    })\n",
    "\n",
    "    # 프롬프트/응답/레코드 저장(프롬프트 “보존”)\n",
    "    mlflow.log_text(PROMPT, \"prompt.txt\")\n",
    "    mlflow.log_text(completion, \"completion.txt\")\n",
    "    mlflow.log_text(json.dumps({\n",
    "        \"prompt\": PROMPT,\n",
    "        \"image_path\": IMAGE_PATH,\n",
    "        \"completion\": completion\n",
    "    }, ensure_ascii=False) + \"\\n\", \"records.jsonl\")\n",
    "\n",
    "    # 입력 이미지도 아티팩트로 보존\n",
    "    tmp_img = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "    image.save(tmp_img.name)\n",
    "    mlflow.log_artifact(tmp_img.name, artifact_path=\"inputs\")\n",
    "\n",
    "    # PyFunc 모델 저장(가중치는 HF에서 로드)\n",
    "    model_cfg = {\"model_id\": MODEL_ID}\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"model\",\n",
    "        python_model=QwenVL_Pyfunc(),\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        pip_requirements=[\n",
    "            \"mlflow>=3.2\",\n",
    "            \"transformers>=4.45\",\n",
    "            \"torch>=2.2\",\n",
    "            \"pillow\",\n",
    "            \"accelerate\",\n",
    "            \"sentencepiece\",\n",
    "            # qwen 유틸은 모델 리포 내 remote_code로 제공\n",
    "        ],\n",
    "        model_config=model_cfg\n",
    "    )\n",
    "\n",
    "    model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "    print(\"Run ID   :\", run.info.run_id)\n",
    "    print(\"Model URI:\", model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d45800-7e61-4c8e-b37f-8dfe6734dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_uri = mlflow-artifacts:/2/models/m-d5f2340df5ee4591ac3efe498aa66eea/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXP_ID = \"2\"\n",
    "M_ID   = \"m-d5f2340df5ee4591ac3efe498aa66eea\"  # mlflow에서 찾을 수 있음\n",
    "model_uri = f\"mlflow-artifacts:/{EXP_ID}/models/{M_ID}/artifacts\"\n",
    "print(\"model_uri =\", model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668c7e5f-9b08-4167-922e-1c07f221d252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31da495b5d2540c7b2c54d3b324a38bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/14 03:36:19 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5007bdb81f8a4f389f039d2904868f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "이미지를 한 문장으로 요약해줘\n",
      "assistant\n",
      "\"나는 당신을 사랑합니다.\" ...\n",
      "latency: 1.1055660247802734\n"
     ]
    }
   ],
   "source": [
    "# 2) 로드\n",
    "loaded = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# 3) 입력(검증/서빙 모두 안전한 base64 형태 권장)\n",
    "img = Image.new(\"RGB\", (64,64), (200,220,240))\n",
    "buf = io.BytesIO(); img.save(buf, format=\"PNG\")\n",
    "img_b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# 4) (옵션) 호출마다 VRAM 반납\n",
    "os.environ[\"PYFUNC_RELEASE_CUDA\"] = \"1\"   # \"1\"이면 predict 끝나고 VRAM 정리\n",
    "os.environ[\"PYFUNC_USE_GPU\"]      = \"auto\" # \"1\"=강제GPU, \"0\"=CPU, \"auto\"=가능하면 GPU\n",
    "\n",
    "# 5) 추론\n",
    "pred = loaded.predict([{\n",
    "    \"prompt\": \"이미지를 한 문장으로 요약해줘\",\n",
    "    \"image_b64\": img_b64,\n",
    "    \"max_new_tokens\": 32\n",
    "}])\n",
    "print(pred.iloc[0][\"completion\"][:120], \"...\")\n",
    "print(\"latency:\", pred.iloc[0][\"latency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a5c43-4152-4b57-b30a-09596c114180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
