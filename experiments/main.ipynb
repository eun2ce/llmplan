{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a4af1f-793f-4ff9-9da0-2002f8b8212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers>=4.53.2 mlflow==3.2.0 accelerate qwen_vl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d957315-49f4-4cd6-80db-1d2fa14d44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, io, json, tempfile, requests\n",
    "from PIL import Image\n",
    "import torch, mlflow\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "import accelerate\n",
    "from importlib import import_module\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"   # HFê°€ TensorFlow ë°±ì—”ë“œ ë¶ˆëŸ¬ì˜¤ì§€ ì•Šë„ë¡\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # TF C++ ë¡œê·¸ ìˆ¨ê¹€ (0~3: info~error)\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"      # (ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´) JAXë„ CPUë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95d3828-f1ae-48ee-bb48-7de3170e2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI = http://mlflow.mlflow.svc.cluster.local:5000\n"
     ]
    }
   ],
   "source": [
    "MLFLOW_URI = \"http://mlflow.mlflow.svc.cluster.local:5000\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_URI\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "print(\"Tracking URI =\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97766e2-c9d5-4496-8999-3e4ec54f3c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/2', creation_time=1755135394244, experiment_id='2', last_update_time=1755135394244, lifecycle_stage='active', name='qwen25vl_demo-eun2ce-20250814-01', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_NAME = \"qwen25vl_demo-eun2ce-20250814-01\"\n",
    "RUN_NAME = \"qwen25vl_smoketest-eun2ce-20250814-01\"\n",
    "# HF_MODEL_ID = \"\"\n",
    "MODEL_NAME = \"/models/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5\"\n",
    "\n",
    "mlflow.set_experiment(EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8065207e-7312-4326-9dfb-48f9e22fcdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_URL = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_images/resolve/main/COCO/000000039769.png\"\n",
    "IMAGE_PATH = \"./testimg.png\"\n",
    "\n",
    "PROMPT = \"ì´ ì´ë¯¸ì§€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜. ê·¸ë¦¬ê³  ì£¼ìš” ê°ì²´ 3ê°œë„ bulletë¡œ ì ì–´ì¤˜.\"\n",
    "\n",
    "def load_image(image_url=None, image_path=None):\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        return Image.open(image_path).convert(\"RGB\")\n",
    "    if image_url:\n",
    "        r = requests.get(image_url, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "    raise ValueError(\"image_url ë˜ëŠ” image_path ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "image = load_image(image_path=IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5626614-d8b2-4c65-afee-9b39750b4a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82da6853164b4b20b03114ab7e7fe75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = os.getenv(\"HF_MODEL_ID\", MODEL_NAME)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "dtype = torch.bfloat16 if use_gpu else torch.float32\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "processor  = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model      = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if use_gpu else None\n",
    ")\n",
    "# Qwenì´ ë°°í¬í•œ ë©€í‹°ëª¨ë‹¬ í—¬í¼ ë¡œë“œ\n",
    "qwen_utils = import_module(\"qwen_vl_utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ee0ba-810e-410b-b1d6-904784a218ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion: system ...\n",
      "latency=3.08s  tokens=198\n"
     ]
    }
   ],
   "source": [
    "def qwen_vl_generate(image: Image.Image, prompt: str, max_new_tokens=128, do_sample=False):\n",
    "    # Qwen í¬ë§·ì˜ chat ë©”ì‹œì§€\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    # text í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # vision ì…ë ¥ ë¶„ë¦¬\n",
    "    image_inputs, video_inputs = qwen_utils.process_vision_info(messages)\n",
    "    # processorë¡œ í…ì„œí™”\n",
    "    inputs = processor(\n",
    "        text=[text], images=image_inputs, videos=video_inputs,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    if use_gpu:\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    t0 = time.time()\n",
    "    gen_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample\n",
    "    )\n",
    "    latency = time.time() - t0\n",
    "\n",
    "    # ì²« í† í°ë¶€í„° ì „ì²´ ë””ì½”ë”© (Qwenì˜ special tokenì€ ìŠ¤í‚µ)\n",
    "    out = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "    # apply_chat_templateê°€ ì•ë¶€ë¶„ê¹Œì§€ í¬í•¨í•˜ë¯€ë¡œ, ë§ˆì§€ë§‰ ì‘ë‹µë§Œ ì˜ë¼ë‚´ê¸°(ê°„ë‹¨ íŒŒì‹±)\n",
    "    # ëŒ€ê°œ ë§ˆì§€ë§‰ \"\\nassistant\\n\" ì´í›„ê°€ ëª¨ë¸ ì‘ë‹µì…ë‹ˆë‹¤. ì•ˆì „í•˜ê²Œ ë’¤ìª½ 1200ìë§Œ ì‚¬ìš©.\n",
    "    completion = out[-1200:].strip()\n",
    "    return completion, latency, len(gen_ids[0]) if hasattr(gen_ids, \"__len__\") else None\n",
    "\n",
    "completion, latency, gen_tokens = qwen_vl_generate(image, PROMPT)\n",
    "print(\"completion:\", completion.splitlines()[0][:120], \"...\")\n",
    "print(f\"latency={latency:.2f}s  tokens={gen_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "102cf55d-959c-46ae-b7dd-f7e9b98ffc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "import base64\n",
    "\n",
    "with open(IMAGE_PATH, \"rb\") as f:\n",
    "    img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "# ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œ ìƒì„± (PyFuncìš©)\n",
    "input_example = {\n",
    "    \"prompt\": \"ì´ë¯¸ì§€ ì† ì¥ë©´ì„ ìš”ì•½í•´ì¤˜.\",\n",
    "    \"image_b64\": img_b64,\n",
    "    \"max_new_tokens\": 128\n",
    "}\n",
    "signature = infer_signature(\n",
    "    model_input=pd.DataFrame([input_example]),\n",
    "    model_output=pd.DataFrame([{\"completion\":\"text\"}])\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch, gc, time, pandas as pd\n",
    "\n",
    "class QwenVL_Pyfunc(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.model_id = context.model_config.get(\"model_id\", \"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "        # ê°€ë²¼ìš´ ê²ƒë§Œ ì¤€ë¹„(ëª¨ë‘ CPU)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, trust_remote_code=True)\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_id, trust_remote_code=True)\n",
    "        # ëª¨ë¸ ë¡œë“œ ì˜µì…˜ì€ ì €ì¥ë§Œ í•´ë‘ê³ , ì§„ì§œ ë¡œë“œëŠ” predictì—ì„œ ë§¤ í˜¸ì¶œë§ˆë‹¤!\n",
    "        self._torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        self._device_map = \"auto\" if torch.cuda.is_available() else None\n",
    "\n",
    "    def _load_image(self, row):\n",
    "        from PIL import Image\n",
    "        import io, requests, base64, os\n",
    "        if \"image_path\" in row and isinstance(row[\"image_path\"], str) and os.path.exists(row[\"image_path\"]):\n",
    "            return Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        if \"image_b64\" in row and row[\"image_b64\"]:\n",
    "            return Image.open(io.BytesIO(base64.b64decode(row[\"image_b64\"]))).convert(\"RGB\")\n",
    "        if \"image_url\" in row and row[\"image_url\"]:\n",
    "            r = requests.get(row[\"image_url\"], timeout=15)\n",
    "            r.raise_for_status()\n",
    "            return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "        raise ValueError(\"image_path / image_url / image_b64 ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        # ë§¤ í˜¸ì¶œë§ˆë‹¤ â€œì ê¹â€ GPUë¡œ ë¡œë“œ\n",
    "        return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            self.model_id,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=self._torch_dtype,\n",
    "            device_map=self._device_map,\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # ì…ë ¥ ì •ê·œí™”\n",
    "        if isinstance(model_input, dict):\n",
    "            rows = [model_input]\n",
    "        elif isinstance(model_input, pd.DataFrame):\n",
    "            rows = model_input.to_dict(orient=\"records\")\n",
    "        else:\n",
    "            raise TypeError(\"predict inputì€ dict ë˜ëŠ” pandas.DataFrame ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        model = self._build_model()\n",
    "        outs = []\n",
    "        try:\n",
    "            for row in rows:\n",
    "                prompt = row.get(\"prompt\", \"\")\n",
    "                image  = self._load_image(row)\n",
    "\n",
    "                # Qwen ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì¤€ë¹„\n",
    "                from importlib import import_module\n",
    "                qwen_utils = import_module(\"qwen_vl_utils\")\n",
    "\n",
    "                messages = [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\",  \"text\": prompt}\n",
    "                    ]\n",
    "                }]\n",
    "                text = self.tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                image_inputs, video_inputs = qwen_utils.process_vision_info(messages)\n",
    "                inputs = self.processor(text=[text], images=image_inputs, videos=video_inputs,\n",
    "                                        return_tensors=\"pt\")\n",
    "\n",
    "                # í•„ìš”í•˜ë©´ë§Œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "                t0 = time.time()\n",
    "                with torch.inference_mode():\n",
    "                    gen_ids = model.generate(**inputs, max_new_tokens=int(row.get(\"max_new_tokens\", 128)))\n",
    "                latency = time.time() - t0\n",
    "\n",
    "                # ì¶œë ¥ í›„ë‹¨ íŒŒì‹±(ê°„ë‹¨)\n",
    "                completion = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "                completion = completion[-1200:].strip()\n",
    "                outs.append({\"completion\": completion, \"latency\": latency})\n",
    "\n",
    "        finally:\n",
    "            # ğŸ”» ë§¤ í˜¸ì¶œ ëë‚  ë•Œ VRAM í•´ì œ\n",
    "            try:\n",
    "                del model\n",
    "            except Exception:\n",
    "                pass\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.ipc_collect()\n",
    "\n",
    "        return pd.DataFrame(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a30cc0d1-2113-4daa-af19-a29098116909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/14 03:15:43 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "2025/08/14 03:15:44 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025/08/14 03:15:44 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c394fa543afc4c58851b805970291c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID   : 87c49955790d47dab31d4919bef18822\n",
      "Model URI: mlflow-artifacts:/2/87c49955790d47dab31d4919bef18822/artifacts/model\n",
      "ğŸƒ View run qwen25vl_smoketest-eun2ce-20250814-01 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/2/runs/87c49955790d47dab31d4919bef18822\n",
      "ğŸ§ª View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    mlflow.log_params({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"dtype\": str(dtype),\n",
    "        \"max_new_tokens\": 128,\n",
    "    })\n",
    "    mlflow.log_metrics({\n",
    "        \"latency_s\": latency,\n",
    "        \"generated_tokens\": gen_tokens or 0,\n",
    "    })\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ/ë ˆì½”ë“œ ì €ì¥(í”„ë¡¬í”„íŠ¸ â€œë³´ì¡´â€)\n",
    "    mlflow.log_text(PROMPT, \"prompt.txt\")\n",
    "    mlflow.log_text(completion, \"completion.txt\")\n",
    "    mlflow.log_text(json.dumps({\n",
    "        \"prompt\": PROMPT,\n",
    "        \"image_path\": IMAGE_PATH,\n",
    "        \"completion\": completion\n",
    "    }, ensure_ascii=False) + \"\\n\", \"records.jsonl\")\n",
    "\n",
    "    # ì…ë ¥ ì´ë¯¸ì§€ë„ ì•„í‹°íŒ©íŠ¸ë¡œ ë³´ì¡´\n",
    "    tmp_img = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "    image.save(tmp_img.name)\n",
    "    mlflow.log_artifact(tmp_img.name, artifact_path=\"inputs\")\n",
    "\n",
    "    # PyFunc ëª¨ë¸ ì €ì¥(ê°€ì¤‘ì¹˜ëŠ” HFì—ì„œ ë¡œë“œ)\n",
    "    model_cfg = {\"model_id\": MODEL_ID}\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"model\",\n",
    "        python_model=QwenVL_Pyfunc(),\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        pip_requirements=[\n",
    "            \"mlflow>=3.2\",\n",
    "            \"transformers>=4.45\",\n",
    "            \"torch>=2.2\",\n",
    "            \"pillow\",\n",
    "            \"accelerate\",\n",
    "            \"sentencepiece\",\n",
    "            # qwen ìœ í‹¸ì€ ëª¨ë¸ ë¦¬í¬ ë‚´ remote_codeë¡œ ì œê³µ\n",
    "        ],\n",
    "        model_config=model_cfg\n",
    "    )\n",
    "\n",
    "    model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "    print(\"Run ID   :\", run.info.run_id)\n",
    "    print(\"Model URI:\", model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d45800-7e61-4c8e-b37f-8dfe6734dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_uri = mlflow-artifacts:/2/models/m-d5f2340df5ee4591ac3efe498aa66eea/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXP_ID = \"2\"\n",
    "M_ID   = \"m-d5f2340df5ee4591ac3efe498aa66eea\"  # mlflowì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŒ\n",
    "model_uri = f\"mlflow-artifacts:/{EXP_ID}/models/{M_ID}/artifacts\"\n",
    "print(\"model_uri =\", model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668c7e5f-9b08-4167-922e-1c07f221d252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31da495b5d2540c7b2c54d3b324a38bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/14 03:36:19 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5007bdb81f8a4f389f039d2904868f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "ì´ë¯¸ì§€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜\n",
      "assistant\n",
      "\"ë‚˜ëŠ” ë‹¹ì‹ ì„ ì‚¬ë‘í•©ë‹ˆë‹¤.\" ...\n",
      "latency: 1.1055660247802734\n"
     ]
    }
   ],
   "source": [
    "# 2) ë¡œë“œ\n",
    "loaded = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# 3) ì…ë ¥(ê²€ì¦/ì„œë¹™ ëª¨ë‘ ì•ˆì „í•œ base64 í˜•íƒœ ê¶Œì¥)\n",
    "img = Image.new(\"RGB\", (64,64), (200,220,240))\n",
    "buf = io.BytesIO(); img.save(buf, format=\"PNG\")\n",
    "img_b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# 4) (ì˜µì…˜) í˜¸ì¶œë§ˆë‹¤ VRAM ë°˜ë‚©\n",
    "os.environ[\"PYFUNC_RELEASE_CUDA\"] = \"1\"   # \"1\"ì´ë©´ predict ëë‚˜ê³  VRAM ì •ë¦¬\n",
    "os.environ[\"PYFUNC_USE_GPU\"]      = \"auto\" # \"1\"=ê°•ì œGPU, \"0\"=CPU, \"auto\"=ê°€ëŠ¥í•˜ë©´ GPU\n",
    "\n",
    "# 5) ì¶”ë¡ \n",
    "pred = loaded.predict([{\n",
    "    \"prompt\": \"ì´ë¯¸ì§€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜\",\n",
    "    \"image_b64\": img_b64,\n",
    "    \"max_new_tokens\": 32\n",
    "}])\n",
    "print(pred.iloc[0][\"completion\"][:120], \"...\")\n",
    "print(\"latency:\", pred.iloc[0][\"latency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f2a5c43-4152-4b57-b30a-09596c114180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- --------------\n",
      "absl-py                            2.1.0\n",
      "accelerate                         1.10.0\n",
      "alembic                            1.14.1\n",
      "altair                             5.5.0\n",
      "annotated-types                    0.7.0\n",
      "anyio                              4.8.0\n",
      "archspec                           0.2.5\n",
      "argon2-cffi                        23.1.0\n",
      "argon2-cffi-bindings               21.2.0\n",
      "arrow                              1.3.0\n",
      "asttokens                          3.0.0\n",
      "astunparse                         1.6.3\n",
      "async_generator                    1.10\n",
      "async-lru                          2.0.4\n",
      "attrs                              25.1.0\n",
      "av                                 15.0.0\n",
      "babel                              2.17.0\n",
      "beautifulsoup4                     4.13.3\n",
      "bleach                             6.2.0\n",
      "blinker                            1.9.0\n",
      "bokeh                              3.6.3\n",
      "boltons                            24.0.0\n",
      "Bottleneck                         1.4.2\n",
      "branca                             0.8.1\n",
      "Brotli                             1.1.0\n",
      "cached-property                    1.5.2\n",
      "cachetools                         5.5.2\n",
      "certifi                            2025.1.31\n",
      "certipy                            0.2.1\n",
      "cffi                               1.17.1\n",
      "charset-normalizer                 3.4.1\n",
      "click                              8.1.8\n",
      "cloudpickle                        3.1.1\n",
      "colorama                           0.4.6\n",
      "comm                               0.2.2\n",
      "conda                              25.1.1\n",
      "conda-libmamba-solver              25.1.1\n",
      "conda-package-handling             2.4.0\n",
      "conda_package_streaming            0.11.0\n",
      "contourpy                          1.3.1\n",
      "cryptography                       44.0.1\n",
      "cycler                             0.12.1\n",
      "Cython                             3.0.12\n",
      "cytoolz                            1.0.1\n",
      "dask                               2025.2.0\n",
      "databricks-sdk                     0.63.0\n",
      "debugpy                            1.8.12\n",
      "decorator                          5.2.1\n",
      "defusedxml                         0.7.1\n",
      "dill                               0.3.9\n",
      "distributed                        2025.2.0\n",
      "distro                             1.9.0\n",
      "docker                             7.1.0\n",
      "et_xmlfile                         2.0.0\n",
      "exceptiongroup                     1.2.2\n",
      "executing                          2.1.0\n",
      "fastapi                            0.116.1\n",
      "fastjsonschema                     2.21.1\n",
      "filelock                           3.13.1\n",
      "Flask                              3.1.1\n",
      "flatbuffers                        25.2.10\n",
      "fonttools                          4.56.0\n",
      "fqdn                               1.5.1\n",
      "frozendict                         2.4.6\n",
      "fsspec                             2025.2.0\n",
      "gast                               0.6.0\n",
      "gitdb                              4.0.12\n",
      "GitPython                          3.1.44\n",
      "gmpy2                              2.1.5\n",
      "google-auth                        2.40.3\n",
      "google-pasta                       0.2.0\n",
      "graphene                           3.4.3\n",
      "graphql-core                       3.2.6\n",
      "graphql-relay                      3.2.0\n",
      "graphviz                           0.20.3\n",
      "greenlet                           3.1.1\n",
      "grpcio                             1.70.0\n",
      "gunicorn                           23.0.0\n",
      "h11                                0.14.0\n",
      "h2                                 4.2.0\n",
      "h5py                               3.13.0\n",
      "hf-xet                             1.1.7\n",
      "hpack                              4.1.0\n",
      "httpcore                           1.0.7\n",
      "httpx                              0.28.1\n",
      "huggingface-hub                    0.34.4\n",
      "hyperframe                         6.1.0\n",
      "idna                               3.10\n",
      "imagecodecs                        2024.12.30\n",
      "imageio                            2.37.0\n",
      "importlib_metadata                 8.6.1\n",
      "importlib_resources                6.5.2\n",
      "iniconfig                          2.0.0\n",
      "ipykernel                          6.29.5\n",
      "ipyleaflet                         0.19.2\n",
      "ipympl                             0.9.6\n",
      "ipython                            8.32.0\n",
      "ipython_genutils                   0.2.0\n",
      "ipywidgets                         8.1.5\n",
      "isoduration                        20.11.0\n",
      "itsdangerous                       2.2.0\n",
      "jedi                               0.19.2\n",
      "Jinja2                             3.1.5\n",
      "joblib                             1.4.2\n",
      "json5                              0.10.0\n",
      "jsonpatch                          1.33\n",
      "jsonpointer                        3.0.0\n",
      "jsonschema                         4.23.0\n",
      "jsonschema-specifications          2024.10.1\n",
      "jupyter_client                     8.6.3\n",
      "jupyter_contrib_core               0.4.2\n",
      "jupyter_contrib_nbextensions       0.7.0\n",
      "jupyter_core                       5.7.2\n",
      "jupyter-events                     0.12.0\n",
      "jupyter-highlight-selected-word    0.2.0\n",
      "jupyter-leaflet                    0.19.2\n",
      "jupyter-lsp                        2.2.5\n",
      "jupyter_nbextensions_configurator  0.6.4\n",
      "jupyter_server                     2.15.0\n",
      "jupyter_server_mathjax             0.2.6\n",
      "jupyter_server_terminals           0.5.3\n",
      "jupyterhub                         5.2.1\n",
      "jupyterlab                         4.3.5\n",
      "jupyterlab_git                     0.51.0\n",
      "jupyterlab_pygments                0.3.0\n",
      "jupyterlab_server                  2.27.3\n",
      "jupyterlab-spellchecker            0.8.4\n",
      "jupyterlab_widgets                 3.0.13\n",
      "keras                              3.8.0\n",
      "kiwisolver                         1.4.8\n",
      "lazy_loader                        0.4\n",
      "libclang                           18.1.1\n",
      "libmambapy                         2.0.5\n",
      "llvmlite                           0.44.0\n",
      "locket                             1.0.0\n",
      "lxml                               5.3.1\n",
      "lz4                                4.3.3\n",
      "Mako                               1.3.9\n",
      "Markdown                           3.7\n",
      "markdown-it-py                     3.0.0\n",
      "MarkupSafe                         3.0.2\n",
      "matplotlib                         3.10.0\n",
      "matplotlib-inline                  0.1.7\n",
      "mdurl                              0.1.2\n",
      "menuinst                           2.2.0\n",
      "mistune                            3.1.2\n",
      "ml-dtypes                          0.4.1\n",
      "mlflow                             3.2.0\n",
      "mlflow-skinny                      3.2.0\n",
      "mlflow-tracing                     3.2.0\n",
      "mpmath                             1.3.0\n",
      "msgpack                            1.1.0\n",
      "munkres                            1.1.4\n",
      "namex                              0.0.8\n",
      "narwhals                           1.27.1\n",
      "nbclassic                          1.2.0\n",
      "nbclient                           0.10.2\n",
      "nbconvert                          7.16.6\n",
      "nbdime                             4.0.2\n",
      "nbformat                           5.10.4\n",
      "nest_asyncio                       1.6.0\n",
      "networkx                           3.4.2\n",
      "notebook                           7.3.2\n",
      "notebook_shim                      0.2.4\n",
      "numba                              0.61.0\n",
      "numexpr                            2.10.2\n",
      "numpy                              2.0.2\n",
      "nvidia-cublas-cu12                 12.6.4.1\n",
      "nvidia-cuda-cupti-cu12             12.6.80\n",
      "nvidia-cuda-nvcc                   11.3.58\n",
      "nvidia-cuda-nvrtc-cu12             12.6.77\n",
      "nvidia-cuda-runtime-cu12           12.6.77\n",
      "nvidia-cudnn-cu12                  9.5.1.17\n",
      "nvidia-cufft-cu12                  11.3.0.4\n",
      "nvidia-curand-cu12                 10.3.7.77\n",
      "nvidia-cusolver-cu12               11.7.1.2\n",
      "nvidia-cusparse-cu12               12.5.4.2\n",
      "nvidia-cusparselt-cu12             0.6.3\n",
      "nvidia-nccl-cu12                   2.21.5\n",
      "nvidia-nvjitlink-cu12              12.6.85\n",
      "nvidia-nvtx-cu12                   12.6.77\n",
      "nvidia-pyindex                     1.0.9\n",
      "oauthlib                           3.2.2\n",
      "openpyxl                           3.1.5\n",
      "opentelemetry-api                  1.36.0\n",
      "opentelemetry-sdk                  1.36.0\n",
      "opentelemetry-semantic-conventions 0.57b0\n",
      "opt_einsum                         3.4.0\n",
      "optree                             0.14.0\n",
      "overrides                          7.7.0\n",
      "packaging                          24.2\n",
      "pamela                             1.2.0\n",
      "pandas                             2.2.3\n",
      "pandocfilters                      1.5.0\n",
      "parso                              0.8.4\n",
      "partd                              1.4.2\n",
      "patsy                              1.0.1\n",
      "pexpect                            4.9.0\n",
      "pickleshare                        0.7.5\n",
      "pillow                             11.1.0\n",
      "pip                                25.0.1\n",
      "pkgutil_resolve_name               1.3.10\n",
      "platformdirs                       4.3.6\n",
      "plotly                             5.24.1\n",
      "pluggy                             1.5.0\n",
      "prometheus_client                  0.21.1\n",
      "prompt_toolkit                     3.0.50\n",
      "protobuf                           5.28.3\n",
      "psutil                             7.0.0\n",
      "ptyprocess                         0.7.0\n",
      "pure_eval                          0.2.3\n",
      "py-cpuinfo                         9.0.0\n",
      "pyarrow                            19.0.1\n",
      "pyasn1                             0.6.1\n",
      "pyasn1_modules                     0.4.2\n",
      "pycosat                            0.6.6\n",
      "pycparser                          2.22\n",
      "pydantic                           2.10.6\n",
      "pydantic_core                      2.27.2\n",
      "Pygments                           2.19.1\n",
      "PyJWT                              2.10.1\n",
      "pyparsing                          3.2.1\n",
      "PySocks                            1.7.1\n",
      "pytest                             8.3.4\n",
      "python-dateutil                    2.9.0.post0\n",
      "python-json-logger                 2.0.7\n",
      "pytz                               2024.1\n",
      "PyWavelets                         1.8.0\n",
      "PyYAML                             6.0.2\n",
      "pyzmq                              26.2.1\n",
      "qwen-vl-utils                      0.0.11\n",
      "referencing                        0.36.2\n",
      "regex                              2025.7.34\n",
      "requests                           2.32.3\n",
      "rfc3339_validator                  0.1.4\n",
      "rfc3986-validator                  0.1.1\n",
      "rich                               13.9.4\n",
      "rpds-py                            0.23.1\n",
      "rsa                                4.9.1\n",
      "ruamel.yaml                        0.18.10\n",
      "ruamel.yaml.clib                   0.2.8\n",
      "safetensors                        0.6.2\n",
      "scikit-image                       0.25.2\n",
      "scikit-learn                       1.6.1\n",
      "scipy                              1.15.2\n",
      "seaborn                            0.13.2\n",
      "Send2Trash                         1.8.3\n",
      "setuptools                         75.8.2\n",
      "six                                1.17.0\n",
      "smmap                              5.0.2\n",
      "sniffio                            1.3.1\n",
      "sortedcontainers                   2.4.0\n",
      "soupsieve                          2.5\n",
      "SQLAlchemy                         2.0.38\n",
      "sqlparse                           0.5.3\n",
      "stack_data                         0.6.3\n",
      "starlette                          0.47.2\n",
      "statsmodels                        0.14.4\n",
      "sympy                              1.13.1\n",
      "tables                             3.10.2\n",
      "tblib                              3.0.0\n",
      "tenacity                           9.0.0\n",
      "tensorboard                        2.18.0\n",
      "tensorboard-data-server            0.7.2\n",
      "tensorflow                         2.18.0\n",
      "termcolor                          2.5.0\n",
      "terminado                          0.18.1\n",
      "threadpoolctl                      3.5.0\n",
      "tifffile                           2025.2.18\n",
      "tinycss2                           1.4.0\n",
      "tokenizers                         0.21.4\n",
      "tomli                              2.2.1\n",
      "toolz                              1.0.0\n",
      "torch                              2.6.0+cu126\n",
      "torchaudio                         2.6.0+cu126\n",
      "torchvision                        0.21.0+cu126\n",
      "tornado                            6.4.2\n",
      "tqdm                               4.67.1\n",
      "traitlets                          5.14.3\n",
      "traittypes                         0.2.1\n",
      "transformers                       4.55.2\n",
      "triton                             3.2.0\n",
      "truststore                         0.10.1\n",
      "types-python-dateutil              2.9.0.20241206\n",
      "typing_extensions                  4.12.2\n",
      "typing_utils                       0.1.0\n",
      "tzdata                             2025.1\n",
      "unicodedata2                       16.0.0\n",
      "uri-template                       1.3.0\n",
      "urllib3                            2.3.0\n",
      "uvicorn                            0.35.0\n",
      "wcwidth                            0.2.13\n",
      "webcolors                          24.11.1\n",
      "webencodings                       0.5.1\n",
      "websocket-client                   1.8.0\n",
      "Werkzeug                           3.1.3\n",
      "wheel                              0.45.1\n",
      "widgetsnbextension                 4.0.13\n",
      "wrapt                              1.17.2\n",
      "xlrd                               2.0.1\n",
      "xyzservices                        2025.1.0\n",
      "zict                               3.0.0\n",
      "zipp                               3.21.0\n",
      "zstandard                          0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ecaf2e-76cf-4c86-9e4e-b7824e3e483e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
