{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a4af1f-793f-4ff9-9da0-2002f8b8212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers>=4.53.2 mlflow==3.2.0 accelerate qwen_vl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d957315-49f4-4cd6-80db-1d2fa14d44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, io, json, tempfile, requests\n",
    "from PIL import Image\n",
    "import torch, mlflow\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "import accelerate\n",
    "from importlib import import_module\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"   # HFê°€ TensorFlow ë°±ì—”ë“œ ë¶ˆëŸ¬ì˜¤ì§€ ì•Šë„ë¡\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # TF C++ ë¡œê·¸ ìˆ¨ê¹€ (0~3: info~error)\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"      # (ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´) JAXë„ CPUë§Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95d3828-f1ae-48ee-bb48-7de3170e2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI = http://mlflow.mlflow.svc.cluster.local:5000\n"
     ]
    }
   ],
   "source": [
    "MLFLOW_URI = \"http://mlflow.mlflow.svc.cluster.local:5000\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_URI\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "print(\"Tracking URI =\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97766e2-c9d5-4496-8999-3e4ec54f3c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/2', creation_time=1755135394244, experiment_id='2', last_update_time=1755135394244, lifecycle_stage='active', name='qwen25vl_demo-eun2ce-20250814-01', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_NAME = \"qwen25vl_demo-eun2ce-20250814-01\"\n",
    "RUN_NAME = \"qwen25vl_smoketest-eun2ce-20250814-01\"\n",
    "# HF_MODEL_ID = \"\"\n",
    "MODEL_NAME = \"/models/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5\"\n",
    "\n",
    "mlflow.set_experiment(EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8065207e-7312-4326-9dfb-48f9e22fcdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_URL = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_images/resolve/main/COCO/000000039769.png\"\n",
    "IMAGE_PATH = \"./testimg.png\"\n",
    "\n",
    "PROMPT = \"ì´ ì´ë¯¸ì§€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜. ê·¸ë¦¬ê³  ì£¼ìš” ê°ì²´ 3ê°œë„ bulletë¡œ ì ì–´ì¤˜.\"\n",
    "\n",
    "def load_image(image_url=None, image_path=None):\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        return Image.open(image_path).convert(\"RGB\")\n",
    "    if image_url:\n",
    "        r = requests.get(image_url, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "    raise ValueError(\"image_url ë˜ëŠ” image_path ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "image = load_image(image_path=IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5626614-d8b2-4c65-afee-9b39750b4a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82da6853164b4b20b03114ab7e7fe75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = os.getenv(\"HF_MODEL_ID\", MODEL_NAME)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "dtype = torch.bfloat16 if use_gpu else torch.float32\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "processor  = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model      = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if use_gpu else None\n",
    ")\n",
    "# Qwenì´ ë°°í¬í•œ ë©€í‹°ëª¨ë‹¬ í—¬í¼ ë¡œë“œ\n",
    "qwen_utils = import_module(\"qwen_vl_utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194ee0ba-810e-410b-b1d6-904784a218ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion: system ...\n",
      "latency=3.08s  tokens=198\n"
     ]
    }
   ],
   "source": [
    "def qwen_vl_generate(image: Image.Image, prompt: str, max_new_tokens=128, do_sample=False):\n",
    "    # Qwen í¬ë§·ì˜ chat ë©”ì‹œì§€\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\",  \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "    # text í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    # vision ì…ë ¥ ë¶„ë¦¬\n",
    "    image_inputs, video_inputs = qwen_utils.process_vision_info(messages)\n",
    "    # processorë¡œ í…ì„œí™”\n",
    "    inputs = processor(\n",
    "        text=[text], images=image_inputs, videos=video_inputs,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    if use_gpu:\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    t0 = time.time()\n",
    "    gen_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample\n",
    "    )\n",
    "    latency = time.time() - t0\n",
    "\n",
    "    # ì²« í† í°ë¶€í„° ì „ì²´ ë””ì½”ë”© (Qwenì˜ special tokenì€ ìŠ¤í‚µ)\n",
    "    out = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "    # apply_chat_templateê°€ ì•ë¶€ë¶„ê¹Œì§€ í¬í•¨í•˜ë¯€ë¡œ, ë§ˆì§€ë§‰ ì‘ë‹µë§Œ ì˜ë¼ë‚´ê¸°(ê°„ë‹¨ íŒŒì‹±)\n",
    "    # ëŒ€ê°œ ë§ˆì§€ë§‰ \"\\nassistant\\n\" ì´í›„ê°€ ëª¨ë¸ ì‘ë‹µì…ë‹ˆë‹¤. ì•ˆì „í•˜ê²Œ ë’¤ìª½ 1200ìë§Œ ì‚¬ìš©.\n",
    "    completion = out[-1200:].strip()\n",
    "    return completion, latency, len(gen_ids[0]) if hasattr(gen_ids, \"__len__\") else None\n",
    "\n",
    "completion, latency, gen_tokens = qwen_vl_generate(image, PROMPT)\n",
    "print(\"completion:\", completion.splitlines()[0][:120], \"...\")\n",
    "print(f\"latency={latency:.2f}s  tokens={gen_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102cf55d-959c-46ae-b7dd-f7e9b98ffc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "import base64\n",
    "\n",
    "with open(IMAGE_PATH, \"rb\") as f:\n",
    "    img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "# ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œ ìƒì„± (PyFuncìš©)\n",
    "input_example = {\n",
    "    \"prompt\": \"ì´ë¯¸ì§€ ì† ì¥ë©´ì„ ìš”ì•½í•´ì¤˜.\",\n",
    "    \"image_b64\": img_b64,\n",
    "    \"max_new_tokens\": 128\n",
    "}\n",
    "signature = infer_signature(\n",
    "    model_input=pd.DataFrame([input_example]),\n",
    "    model_output=pd.DataFrame([{\"completion\":\"text\"}])\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch, gc, time, pandas as pd\n",
    "\n",
    "class QwenVL_Pyfunc(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.model_id = context.model_config.get(\"model_id\", \"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "        # ê°€ë²¼ìš´ ê²ƒë§Œ ì¤€ë¹„(ëª¨ë‘ CPU)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, trust_remote_code=True)\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_id, trust_remote_code=True)\n",
    "        # ëª¨ë¸ ë¡œë“œ ì˜µì…˜ì€ ì €ì¥ë§Œ í•´ë‘ê³ , ì§„ì§œ ë¡œë“œëŠ” predictì—ì„œ ë§¤ í˜¸ì¶œë§ˆë‹¤!\n",
    "        self._torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        self._device_map = \"auto\" if torch.cuda.is_available() else None\n",
    "\n",
    "    def _load_image(self, row):\n",
    "        from PIL import Image\n",
    "        import io, requests, base64, os\n",
    "        if \"image_path\" in row and isinstance(row[\"image_path\"], str) and os.path.exists(row[\"image_path\"]):\n",
    "            return Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        if \"image_b64\" in row and row[\"image_b64\"]:\n",
    "            return Image.open(io.BytesIO(base64.b64decode(row[\"image_b64\"]))).convert(\"RGB\")\n",
    "        if \"image_url\" in row and row[\"image_url\"]:\n",
    "            r = requests.get(row[\"image_url\"], timeout=15)\n",
    "            r.raise_for_status()\n",
    "            return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "        raise ValueError(\"image_path / image_url / image_b64 ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        # ë§¤ í˜¸ì¶œë§ˆë‹¤ â€œì ê¹â€ GPUë¡œ ë¡œë“œ\n",
    "        return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            self.model_id,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=self._torch_dtype,\n",
    "            device_map=self._device_map,\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # ì…ë ¥ ì •ê·œí™”\n",
    "        if isinstance(model_input, dict):\n",
    "            rows = [model_input]\n",
    "        elif isinstance(model_input, pd.DataFrame):\n",
    "            rows = model_input.to_dict(orient=\"records\")\n",
    "        else:\n",
    "            raise TypeError(\"predict inputì€ dict ë˜ëŠ” pandas.DataFrame ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        model = self._build_model()\n",
    "        outs = []\n",
    "        try:\n",
    "            for row in rows:\n",
    "                prompt = row.get(\"prompt\", \"\")\n",
    "                image  = self._load_image(row)\n",
    "\n",
    "                # Qwen ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì¤€ë¹„\n",
    "                from importlib import import_module\n",
    "                qwen_utils = import_module(\"qwen_vl_utils\")\n",
    "\n",
    "                messages = [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\",  \"text\": prompt}\n",
    "                    ]\n",
    "                }]\n",
    "                text = self.tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                image_inputs, video_inputs = qwen_utils.process_vision_info(messages)\n",
    "                inputs = self.processor(text=[text], images=image_inputs, videos=video_inputs,\n",
    "                                        return_tensors=\"pt\")\n",
    "\n",
    "                # í•„ìš”í•˜ë©´ë§Œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "                t0 = time.time()\n",
    "                with torch.inference_mode():\n",
    "                    gen_ids = model.generate(**inputs, max_new_tokens=int(row.get(\"max_new_tokens\", 128)))\n",
    "                latency = time.time() - t0\n",
    "\n",
    "                # ì¶œë ¥ í›„ë‹¨ íŒŒì‹±(ê°„ë‹¨)\n",
    "                completion = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "                completion = completion[-1200:].strip()\n",
    "                outs.append({\"completion\": completion, \"latency\": latency})\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                del model\n",
    "            except Exception:\n",
    "                pass\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.ipc_collect()\n",
    "\n",
    "        return pd.DataFrame(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a30cc0d1-2113-4daa-af19-a29098116909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/14 03:15:43 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "2025/08/14 03:15:44 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2025/08/14 03:15:44 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c394fa543afc4c58851b805970291c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID   : 87c49955790d47dab31d4919bef18822\n",
      "Model URI: mlflow-artifacts:/2/87c49955790d47dab31d4919bef18822/artifacts/model\n",
      "ğŸƒ View run qwen25vl_smoketest-eun2ce-20250814-01 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/2/runs/87c49955790d47dab31d4919bef18822\n",
      "ğŸ§ª View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "    mlflow.log_params({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"dtype\": str(dtype),\n",
    "        \"max_new_tokens\": 128,\n",
    "    })\n",
    "    mlflow.log_metrics({\n",
    "        \"latency_s\": latency,\n",
    "        \"generated_tokens\": gen_tokens or 0,\n",
    "    })\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ/ë ˆì½”ë“œ ì €ì¥(í”„ë¡¬í”„íŠ¸ â€œë³´ì¡´â€)\n",
    "    mlflow.log_text(PROMPT, \"prompt.txt\")\n",
    "    mlflow.log_text(completion, \"completion.txt\")\n",
    "    mlflow.log_text(json.dumps({\n",
    "        \"prompt\": PROMPT,\n",
    "        \"image_path\": IMAGE_PATH,\n",
    "        \"completion\": completion\n",
    "    }, ensure_ascii=False) + \"\\n\", \"records.jsonl\")\n",
    "\n",
    "    # ì…ë ¥ ì´ë¯¸ì§€ë„ ì•„í‹°íŒ©íŠ¸ë¡œ ë³´ì¡´\n",
    "    tmp_img = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "    image.save(tmp_img.name)\n",
    "    mlflow.log_artifact(tmp_img.name, artifact_path=\"inputs\")\n",
    "\n",
    "    # PyFunc ëª¨ë¸ ì €ì¥(ê°€ì¤‘ì¹˜ëŠ” HFì—ì„œ ë¡œë“œ)\n",
    "    model_cfg = {\"model_id\": MODEL_ID}\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"model\",\n",
    "        python_model=QwenVL_Pyfunc(),\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        pip_requirements=[\n",
    "            \"mlflow>=3.2\",\n",
    "            \"transformers>=4.45\",\n",
    "            \"torch>=2.2\",\n",
    "            \"pillow\",\n",
    "            \"accelerate\",\n",
    "            \"sentencepiece\",\n",
    "            # qwen ìœ í‹¸ì€ ëª¨ë¸ ë¦¬í¬ ë‚´ remote_codeë¡œ ì œê³µ\n",
    "        ],\n",
    "        model_config=model_cfg\n",
    "    )\n",
    "\n",
    "    model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "    print(\"Run ID   :\", run.info.run_id)\n",
    "    print(\"Model URI:\", model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d45800-7e61-4c8e-b37f-8dfe6734dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_uri = mlflow-artifacts:/2/models/m-d5f2340df5ee4591ac3efe498aa66eea/artifacts\n"
     ]
    }
   ],
   "source": [
    "EXP_ID = \"2\"\n",
    "M_ID   = \"m-d5f2340df5ee4591ac3efe498aa66eea\"  # mlflowì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŒ\n",
    "model_uri = f\"mlflow-artifacts:/{EXP_ID}/models/{M_ID}/artifacts\"\n",
    "print(\"model_uri =\", model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668c7e5f-9b08-4167-922e-1c07f221d252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31da495b5d2540c7b2c54d3b324a38bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/14 03:36:19 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - sentencepiece (current: uninstalled, required: sentencepiece)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5007bdb81f8a4f389f039d2904868f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "ì´ë¯¸ì§€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜\n",
      "assistant\n",
      "\"ë‚˜ëŠ” ë‹¹ì‹ ì„ ì‚¬ë‘í•©ë‹ˆë‹¤.\" ...\n",
      "latency: 1.1055660247802734\n"
     ]
    }
   ],
   "source": [
    "# 2) ë¡œë“œ\n",
    "loaded = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# 3) ì…ë ¥(ê²€ì¦/ì„œë¹™ ëª¨ë‘ ì•ˆì „í•œ base64 í˜•íƒœ ê¶Œì¥)\n",
    "img = Image.new(\"RGB\", (64,64), (200,220,240))\n",
    "buf = io.BytesIO(); img.save(buf, format=\"PNG\")\n",
    "img_b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# 4) (ì˜µì…˜) í˜¸ì¶œë§ˆë‹¤ VRAM ë°˜ë‚©\n",
    "os.environ[\"PYFUNC_RELEASE_CUDA\"] = \"1\"   # \"1\"ì´ë©´ predict ëë‚˜ê³  VRAM ì •ë¦¬\n",
    "os.environ[\"PYFUNC_USE_GPU\"]      = \"auto\" # \"1\"=ê°•ì œGPU, \"0\"=CPU, \"auto\"=ê°€ëŠ¥í•˜ë©´ GPU\n",
    "\n",
    "# 5) ì¶”ë¡ \n",
    "pred = loaded.predict([{\n",
    "    \"prompt\": \"ì´ë¯¸ì§€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜\",\n",
    "    \"image_b64\": img_b64,\n",
    "    \"max_new_tokens\": 32\n",
    "}])\n",
    "print(pred.iloc[0][\"completion\"][:120], \"...\")\n",
    "print(\"latency:\", pred.iloc[0][\"latency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a5c43-4152-4b57-b30a-09596c114180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
