{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296e9ff1-4f9a-455a-8d73-b9457f602f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"transformers==4.53.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0aaadbb-7653-4da4-8fc2-8c501075a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 00:19:56.799374: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755649196.810653    7055 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755649196.814066    7055 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-20 00:19:56.826982: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re, os, json\n",
    "import base64\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel, AutoImageProcessor\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cbe6a8a-a32e-495b-a6ed-46ac451d8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"xlangai/OpenCUA-7B\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGE_DIR = Path(\"./images/1921c65e-38c4-4aec-a298-f7d8a8635197\")\n",
    "\n",
    "FOLDER_NAME = IMAGE_DIR.name\n",
    "RESULT_DIR = Path(\"results\") / FOLDER_NAME\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CAPS_JSON = RESULT_DIR / f\"{FOLDER_NAME}_captions.json\"\n",
    "ACTS_JSON = RESULT_DIR / f\"{FOLDER_NAME}_actions.json\"\n",
    "\n",
    "DO_ACTIONS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c304474e-bad8-4f35-a249-b6f682a02c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "def free_memory():\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "\n",
    "_num_re = re.compile(r'(\\d+)')\n",
    "def natural_key(path: Path):\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in _num_re.split(path.name)]\n",
    "\n",
    "def collect_images(root: Path) -> List[Path]:\n",
    "    exts = {\".png\", \".jpg\", \".jpeg\"}\n",
    "    return sorted([p for p in root.glob(\"*\") if p.suffix.lower() in exts], key=natural_key)\n",
    "\n",
    "URL_RE = re.compile(r'\\bhttps?://[^\\s]+|\\bwww\\.[^\\s]+\\b', re.IGNORECASE)\n",
    "def extract_urls(text: str) -> List[str]:\n",
    "    return URL_RE.findall(text or \"\")\n",
    "\n",
    "def clamp01(x: float) -> float:\n",
    "    try:\n",
    "        v = float(x)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    return 0.0 if v < 0 else 1.0 if v > 1 else v\n",
    "\n",
    "def normalize_xy(xy, W, H):\n",
    "    \"\"\"\n",
    "    [x,y]가 픽셀이면 0..1로 변환, 이미 비율이면 클램프만\n",
    "    \"\"\"\n",
    "    if isinstance(xy, (list, tuple)) and len(xy) == 2:\n",
    "        x, y = xy\n",
    "        if isinstance(x, (int, float)) and isinstance(y, (int, float)):\n",
    "            if x > 1 or y > 1:  # 픽셀\n",
    "                return [round(clamp01(x / max(W, 1)), 4), round(clamp01(y / max(H, 1)), 4)]\n",
    "            return [round(clamp01(x), 4), round(clamp01(y), 4)]\n",
    "    return None\n",
    "\n",
    "def parse_actions_json(text: str):\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    # [[{...}]] 형태 평탄화\n",
    "    if isinstance(obj, list) and len(obj) == 1 and isinstance(obj[0], list):\n",
    "        obj = obj[0]\n",
    "    return obj if isinstance(obj, list) else None\n",
    "\n",
    "def history_to_text(history: List[Dict[str, Any]], max_steps: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    최근 max_steps 단계만 간단 요약 텍스트로.\n",
    "    history 항목: {\"caption\": str, \"actions\": list[dict]}\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    take = history[-max_steps:]\n",
    "    for i, h in enumerate(take, 1):\n",
    "        caps = (h.get(\"caption\") or \"\")[:120].replace(\"\\n\", \" \")\n",
    "        acts = h.get(\"actions\") or []\n",
    "        # 간단 요약\n",
    "        act_summ = []\n",
    "        for a in acts:\n",
    "            if not isinstance(a, dict): continue\n",
    "            t = a.get(\"type\")\n",
    "            if t in (\"click\", \"move\", \"drag\"):\n",
    "                act_summ.append(f'{t}@{a.get(\"xy\", \"\")}')\n",
    "            elif t == \"type\":\n",
    "                txt = str(a.get(\"text\",\"\"))[:40]\n",
    "                act_summ.append(f'type(\"{txt}\")')\n",
    "            elif t == \"key\":\n",
    "                act_summ.append(f'key{a.get(\"keys\", [])}')\n",
    "            else:\n",
    "                act_summ.append(t or \"?\")\n",
    "        lines.append(f\"- Step{-len(take)+i+len(history)}: caption={caps} | actions=[{', '.join(act_summ)}]\")\n",
    "    return \"\\n\".join(lines) if lines else \"(no history)\"\n",
    "\n",
    "def history_has_url_enter(history: List[Dict[str, Any]], url: str) -> bool:\n",
    "    \"\"\"\n",
    "    과거 스텝에 url을 입력(type)하고 enter까지 친 기록이 있는가?\n",
    "    \"\"\"\n",
    "    typed = False\n",
    "    for h in history:\n",
    "        acts = h.get(\"actions\") or []\n",
    "        for a in acts:\n",
    "            if not isinstance(a, dict): continue\n",
    "            if a.get(\"type\") == \"type\" and isinstance(a.get(\"text\"), str) and url in a[\"text\"]:\n",
    "                typed = True\n",
    "            if typed and a.get(\"type\") == \"key\" and \"enter\" in (a.get(\"keys\") or []):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0776c2-0af3-4f17-90c4-3cf46739711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd37e537b85e4c369121fa3c443f57aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "print(\"Model loaded on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a7f647-ad15-4b65-8941-f74c26a4ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(img_path: str) -> str:\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    processed = image_processor(image, return_tensors=\"pt\")\n",
    "    pixel_values = processed['pixel_values'].to(device=device, dtype=torch.bfloat16)\n",
    "    grid_thws = processed.get('image_grid_thw')\n",
    "    if grid_thws is not None:\n",
    "        grid_thws = grid_thws.to(device=device)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes GUI screenshots.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": f\"data:image/png;base64,{encode_image(img_path)}\"},\n",
    "            {\"type\": \"text\",  \"text\": \"Describe the content of this screenshot briefly.\"},\n",
    "        ]},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
    "    input_ids = torch.tensor([input_ids], device=device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        pixel_values=pixel_values,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    if grid_thws is not None:\n",
    "        gen_kwargs[\"grid_thws\"] = grid_thws\n",
    "\n",
    "    out_ids = model.generate(**gen_kwargs)\n",
    "    out_ids = out_ids[:, input_ids.shape[1]:]\n",
    "    text = tokenizer.batch_decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0].strip()\n",
    "    free_memory()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f42e79b-f1b7-426d-b443-051c07d709d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_actions(img_path: str, instruction: str, caption: str, history: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    히스토리(이전 캡션/액션 요약)를 주고 '다음 단계'만 JSON으로 생성.\n",
    "    - 좌표는 반드시 0..1 (소수점 4자리)\n",
    "    - instruction 내 URL은 '한 번만' type -> enter (히스토리에 있으면 반복 금지)\n",
    "    \"\"\"\n",
    "    with Image.open(img_path) as im:\n",
    "        W, H = im.size\n",
    "\n",
    "    urls = extract_urls(instruction)\n",
    "    url_hint = f\"EXTRACTED_URLS: {urls}\" if urls else \"EXTRACTED_URLS: []\"\n",
    "    already_navigated = False\n",
    "    if urls:\n",
    "        already_navigated = history_has_url_enter(history, urls[0])\n",
    "\n",
    "    hist_text = history_to_text(history, max_steps=6)\n",
    "\n",
    "    rules = f\"\"\"\n",
    "You MUST output ONLY a JSON array of the NEXT steps (not the whole task).\n",
    "Use these fields:\n",
    "- \"type\": one of [\"move\",\"click\",\"double_click\",\"right_click\",\"drag\",\"scroll\",\"type\",\"key\",\"wait\"]\n",
    "- \"xy\": [x_rel,y_rel] in [0,1], 4 decimals (pointer actions only)\n",
    "- \"delta\": for scroll, e.g., {{\"dx\":0,\"dy\":-0.2}}\n",
    "- \"text\": for type\n",
    "- \"keys\": for key (e.g., [\"enter\"])\n",
    "\n",
    "Rules:\n",
    "- Use ONLY relative coordinates (0..1). NEVER absolute pixels.\n",
    "- Do NOT repeat past actions unless strictly needed.\n",
    "- { \"If instruction includes a URL, and it has NOT been typed+entered before, include a 'type' with the exact URL then a 'key' with ['enter'].\" if not already_navigated else \"URL typing+enter already done earlier. Do NOT type the URL again.\"}\n",
    "- Return ONLY JSON. No explanations.\n",
    "- Prefer concise steps (<= 3) for the next move.\n",
    "\n",
    "{url_hint}\n",
    "Image size hint: width={W}, height={H}.\n",
    "\n",
    "History (most recent last):\n",
    "{hist_text}\n",
    "\n",
    "Current screen description:\n",
    "{caption}\n",
    "\n",
    "Instruction (global goal):\n",
    "{instruction}\n",
    "\"\"\".strip()\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a reliable GUI agent. Output ONLY JSON of the NEXT steps using relative coordinates.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": f\"data:image/png;base64,{encode_image(img_path)}\"},\n",
    "            {\"type\": \"text\",  \"text\": rules}\n",
    "        ]},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
    "    input_ids = torch.tensor([input_ids], device=device)\n",
    "\n",
    "    processed = image_processor(Image.open(img_path).convert('RGB'), return_tensors=\"pt\")\n",
    "    pixel_values = processed['pixel_values'].to(device=device, dtype=torch.bfloat16)\n",
    "    grid_thws = processed.get('image_grid_thw')\n",
    "    if grid_thws is not None:\n",
    "        grid_thws = grid_thws.to(device=device)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        pixel_values=pixel_values,\n",
    "        max_new_tokens=384,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    if grid_thws is not None:\n",
    "        gen_kwargs[\"grid_thws\"] = grid_thws\n",
    "\n",
    "    out_ids = model.generate(**gen_kwargs)\n",
    "    out_ids = out_ids[:, input_ids.shape[1]:]\n",
    "    text = tokenizer.batch_decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0].strip()\n",
    "\n",
    "    # ---- 후처리: JSON 파싱 & 좌표 정규화 ----\n",
    "    steps = parse_actions_json(text) or []\n",
    "    norm_steps = []\n",
    "    for s in steps:\n",
    "        if not isinstance(s, dict): continue\n",
    "        rec = {\"type\": s.get(\"type\",\"\")}\n",
    "        if \"xy\" in s:\n",
    "            xy = normalize_xy(s[\"xy\"], W, H)\n",
    "            if xy: rec[\"xy\"] = xy\n",
    "        if \"delta\" in s: rec[\"delta\"] = s[\"delta\"]\n",
    "        if \"text\"  in s: rec[\"text\"]  = s[\"text\"]\n",
    "        if \"keys\"  in s: rec[\"keys\"]  = s[\"keys\"]\n",
    "        if rec[\"type\"]: norm_steps.append(rec)\n",
    "\n",
    "    # URL 중복 방지(히스토리에 이미 url+enter가 있으면 제거)\n",
    "    if urls and already_navigated:\n",
    "        url0 = urls[0]\n",
    "        filtered = []\n",
    "        for a in norm_steps:\n",
    "            if a.get(\"type\") == \"type\" and isinstance(a.get(\"text\"), str) and url0 in a[\"text\"]:\n",
    "                continue\n",
    "            if a.get(\"type\") == \"key\" and \"enter\" in (a.get(\"keys\") or []):\n",
    "                continue\n",
    "            filtered.append(a)\n",
    "        norm_steps = filtered\n",
    "\n",
    "    free_memory()\n",
    "    return json.dumps(norm_steps, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42f99a9-64f4-4552-b12d-939a56862223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_images(image_root: Path, instruction: str):\n",
    "    images = collect_images(image_root)\n",
    "    print(f\"Found {len(images)} image(s) under {image_root.resolve()}\")\n",
    "\n",
    "    captions_acc: List[Dict[str, Any]] = []\n",
    "    actions_acc:  List[Dict[str, Any]] = []\n",
    "    history:      List[Dict[str, Any]] = []  # [{caption, actions(list[dict])}, ...]\n",
    "\n",
    "    for img_path in images:\n",
    "        rel = img_path.relative_to(image_root)\n",
    "        print(f\"\\n[Caption] {rel}\")\n",
    "        caption = generate_image_caption(str(img_path))\n",
    "        captions_acc.append({\"image\": str(img_path), \"caption\": caption})\n",
    "        CAPS_JSON.write_text(json.dumps(captions_acc, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "        if DO_ACTIONS:\n",
    "            print(f\"[Actions] {rel}\")\n",
    "            actions_json_str = generate_actions(str(img_path), instruction, caption, history)\n",
    "\n",
    "            # 저장용/히스토리용 파싱 (파싱 실패 시 빈 리스트)\n",
    "            try:\n",
    "                parsed_actions = json.loads(actions_json_str)\n",
    "                if not isinstance(parsed_actions, list):\n",
    "                    parsed_actions = []\n",
    "            except json.JSONDecodeError:\n",
    "                parsed_actions = []\n",
    "\n",
    "            actions_acc.append({\n",
    "                \"image\": str(img_path),\n",
    "                \"instruction\": instruction,\n",
    "                \"caption\": caption,\n",
    "                \"actions\": parsed_actions\n",
    "            })\n",
    "            ACTS_JSON.write_text(json.dumps(actions_acc, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "            # 히스토리 업데이트\n",
    "            history.append({\n",
    "                \"caption\": caption,\n",
    "                \"actions\": parsed_actions\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00317f28-1e3b-4bbe-b89e-05e46c0926f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 image(s) under /home/jovyan/images/1921c65e-38c4-4aec-a298-f7d8a8635197\n",
      "\n",
      "[Caption] 1921c65e-38c4-4aec-a298-f7d8a8635197_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Actions] 1921c65e-38c4-4aec-a298-f7d8a8635197_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Caption] 1921c65e-38c4-4aec-a298-f7d8a8635197_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Actions] 1921c65e-38c4-4aec-a298-f7d8a8635197_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Caption] 1921c65e-38c4-4aec-a298-f7d8a8635197_3.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Actions] 1921c65e-38c4-4aec-a298-f7d8a8635197_3.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Caption] 1921c65e-38c4-4aec-a298-f7d8a8635197_4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Actions] 1921c65e-38c4-4aec-a298-f7d8a8635197_4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Caption] 1921c65e-38c4-4aec-a298-f7d8a8635197_5.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151644 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Actions] 1921c65e-38c4-4aec-a298-f7d8a8635197_5.jpg\n",
      "CPU times: user 24.8 s, sys: 1.36 s, total: 26.1 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "instruction = \"www.xxx.com 웹 사이트에 접속해줘\"\n",
    "process_all_images(IMAGE_DIR, instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ae092-7d5c-4ed6-a207-cd47ee3fd56e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
